# Antigravity Chatbot Resolution Plan

## 1. Diagnosis Phase

### Initial Assessment
-   **Symptom**: User reports "bot not working" (queries not accepted or no response).
-   **Investigation**:
    -   Verified API Endpoint: `http://localhost:8000/api/chat` returned `404 Not Found`.
    -   Verified Route Configuration: `api/index.py` correctly defines the route.
    -   **Root Cause Identified**: The local server running on port 8000 was a "zombie" process serving an outdated version of the application code. This prevented new code changes (including the API routes) from taking effect.
    -   **Secondary Findings**:
        -   `api/ai_engine.py` had a syntax error (missing `try` block), which would have caused the newer server to crash on startup, confusing the debugging process.
        -   Scientific notation in responses was fixed in config but required server restart to apply.

## 2. Verification Phase

| Component | Status | Verification Method | Fix Applied |
| :--- | :--- | :--- | :--- |
| **Frontend** | ‚úÖ Pass | `index.html` loads and UI is interactive. | N/A |
| **Backend API** | ‚úÖ Pass | `test_bot_endpoint.py` (after server restart). | Routes confirmed via `debug_routes.py`. |
| **LLM Connection** | ‚ö†Ô∏è Fallback | `ai_engine.py` connectivity check. | Added graceful fallback to "Local Regex Mode" if Ollama is down. |
| **Runtime Env** | üîÑ Fixed | Port 8000 conflict detection. | Killed 8+ zombie python processes. |

## 3. Minimum Working Bot Checklist

To guarantee a working chatbot, ensure these conditions are met:

- [x] **Repo State**: Clean git status (or equivalent).
- [x] **Dependencies**: `fastapi`, `uvicorn`, `requests`, `langchain-ollama` installed.
- [x] **No Zombie Processes**: Port 8000 must be free before starting (`kill_server.py`).
- [x] **Valid Syntax**: `api/ai_engine.py` must parse correctly (no `IndentationError` or `SyntaxError`).
- [x] **Routes Registered**: `api/index.py` must import `app` successfully.
- [x] **Server Running**: `run_local_server.bat` must be active.

## 4. Deployment Plan

### A. Pre-Deploy
1.  **Sanitize Environment**:
    ```powershell
    python kill_server.py
    ```
2.  **Verify Configuration**:
    -   Ensure `.env` exists (optional for local mode fallback).
    -   Ensure `requirements.txt` is installed: `pip install -r requirements.txt`.

### B. Deployment Command (Local)
Use the dedicated launcher which handles venv activation and uvicorn:
```powershell
run_local_server.bat
```

### C. Deployment Command (Cloud/Vercel)
Since local LLM (Ollama) cannot run on Vercel:
1.  **Build**: Vercel handles this via `requirements.txt`.
2.  **Environment Variables**: Ensure `GROQ_API_KEY` is set in Vercel Project Settings if using cloud AI.
3.  **Deploy**:
    ```powershell
    vercel --prod
    ```

### D. Post-Deploy Smoke Test
1.  Open Dashboard.
2.  Type "Hello".
3.  **Expected**: Response should appear (either from AI or "Pashu Sahayak (Local Mode)").
4.  Type "Who has the most Paddy?".
5.  **Expected**: Data-driven response with correct number formatting (no scientific notation).

---
*Report generated by Antigravity AI Systems Specialist.*
